{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d2f80e",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "Import dependencies and verify configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d180f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from aponyx.config import DATA_DIR, LOGS_DIR, SUITABILITY_REGISTRY_PATH, EVALUATION_DIR\n",
    "from aponyx.data import fetch_cdx\n",
    "from aponyx.data.sources import BloombergSource\n",
    "from aponyx.persistence import save_json\n",
    "from aponyx.evaluation.suitability import (\n",
    "    evaluate_signal_suitability,\n",
    "    SuitabilityConfig,\n",
    "    SuitabilityRegistry,\n",
    "    generate_suitability_report,\n",
    "    save_report,\n",
    ")\n",
    "\n",
    "# Configure logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SIGNAL SUITABILITY EVALUATION WORKFLOW — Step 3 of 5\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Logs directory: {LOGS_DIR}\")\n",
    "print(f\"  Evaluation directory: {EVALUATION_DIR}\")\n",
    "print(f\"  Registry path: {SUITABILITY_REGISTRY_PATH}\")\n",
    "print(f\"\\n✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3f445",
   "metadata": {},
   "source": [
    "## 2. Load Signals from Step 2\n",
    "\n",
    "Load computed signals DataFrame from previous workflow step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for signals file\n",
    "signals_path = DATA_DIR / \"processed\" / \"signals.parquet\"\n",
    "\n",
    "if not signals_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No signals found at {signals_path}.\\n\"\n",
    "        \"Run 02_signal_computation.ipynb first to compute signals.\"\n",
    "    )\n",
    "\n",
    "# Load signals\n",
    "signals = pd.read_parquet(signals_path)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SIGNALS LOADED\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"File: {signals_path}\")\n",
    "print(f\"Shape: {signals.shape}\")\n",
    "print(f\"Columns: {list(signals.columns)}\")\n",
    "print(f\"Date range: {signals.index.min()} to {signals.index.max()}\")\n",
    "print(f\"Total observations: {len(signals)}\")\n",
    "\n",
    "# Display summary statistics\n",
    "summary_data = []\n",
    "for col in signals.columns:\n",
    "    summary_data.append({\n",
    "        'Signal': col,\n",
    "        'Valid Obs': signals[col].notna().sum(),\n",
    "        'Mean': f\"{signals[col].mean():.3f}\",\n",
    "        'Std': f\"{signals[col].std():.3f}\",\n",
    "        'Min': f\"{signals[col].min():.2f}\",\n",
    "        'Max': f\"{signals[col].max():.2f}\",\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(f\"\\nSignal Summary:\\n\")\n",
    "print(summary_df.to_markdown(index=False))\n",
    "\n",
    "print(f\"\\n✓ Signals loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892eae7",
   "metadata": {},
   "source": [
    "## 3. Load Target Product Data\n",
    "\n",
    "Load CDX spread data as the evaluation target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"LOADING TARGET PRODUCT DATA\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Initialize source\n",
    "source = BloombergSource()\n",
    "\n",
    "# Fetch CDX data\n",
    "print(\"Loading CDX IG 5Y spreads...\")\n",
    "cdx_df = fetch_cdx(\n",
    "    source=source,\n",
    "    security=\"cdx_ig_5y\",\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# Extract spread column\n",
    "cdx_spread = cdx_df['spread']\n",
    "\n",
    "print(f\"✓ Loaded CDX IG 5Y: {len(cdx_spread)} rows\")\n",
    "print(f\"  Date range: {cdx_spread.index.min()} to {cdx_spread.index.max()}\")\n",
    "print(f\"  Missing values: {cdx_spread.isna().sum()}\")\n",
    "\n",
    "# Verify alignment with signals\n",
    "aligned = signals.index.equals(cdx_spread.index)\n",
    "if aligned:\n",
    "    print(f\"\\n✓ Target aligned with signals index ({len(signals)} dates)\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Target index differs from signals\")\n",
    "    print(f\"  Signals dates: {len(signals)}\")\n",
    "    print(f\"  Target dates: {len(cdx_spread)}\")\n",
    "    print(f\"  Evaluation will use aligned subset\")\n",
    "\n",
    "print(f\"\\n✓ Target product data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792c444",
   "metadata": {},
   "source": [
    "## 4. Configure Evaluation Parameters\n",
    "\n",
    "Set parameters for suitability evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum observations (1 trading year for tactical signals)\n",
    "min_obs = 252\n",
    "\n",
    "# Create configuration\n",
    "config = SuitabilityConfig(\n",
    "    lags=[1, 3, 5],\n",
    "    min_obs=min_obs,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EVALUATION CONFIGURATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Display configuration\n",
    "config_data = [\n",
    "    {'Parameter': 'Forecast Lags', 'Value': str(config.lags)},\n",
    "    {'Parameter': 'Minimum Observations', 'Value': str(config.min_obs)},\n",
    "    {'Parameter': 'PASS Threshold', 'Value': f\"{config.pass_threshold:.2f}\"},\n",
    "    {'Parameter': 'HOLD Threshold', 'Value': f\"{config.hold_threshold:.2f}\"},\n",
    "    {'Parameter': 'Data Health Weight', 'Value': f\"{config.data_health_weight:.1%}\"},\n",
    "    {'Parameter': 'Predictive Weight', 'Value': f\"{config.predictive_weight:.1%}\"},\n",
    "    {'Parameter': 'Economic Weight', 'Value': f\"{config.economic_weight:.1%}\"},\n",
    "    {'Parameter': 'Stability Weight', 'Value': f\"{config.stability_weight:.1%}\"},\n",
    "]\n",
    "\n",
    "config_df = pd.DataFrame(config_data)\n",
    "print(config_df.to_markdown(index=False))\n",
    "\n",
    "print(f\"\\n\\nRationale:\")\n",
    "print(f\"  - Lags {config.lags}: Test 1-, 3-, and 5-day forecast horizons\")\n",
    "print(f\"  - Min obs {config.min_obs}: Requires 1 trading year for statistical validity\")\n",
    "print(f\"  - Component weights: Emphasis on predictive power (40%)\")\n",
    "print(f\"  - Decision thresholds: PASS ≥0.7, HOLD 0.4-0.7, FAIL <0.4\")\n",
    "\n",
    "print(f\"\\n✓ Configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b623d",
   "metadata": {},
   "source": [
    "## 5. Evaluate Signal-Product Suitability\n",
    "\n",
    "Run suitability evaluation for each signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14840d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EVALUATING SIGNAL SUITABILITY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Store results\n",
    "results_dict = {}\n",
    "\n",
    "# Evaluate each signal (fail-fast: no try/except)\n",
    "for signal_name in signals.columns:\n",
    "    print(f\"Evaluating {signal_name}...\")\n",
    "    \n",
    "    result = evaluate_signal_suitability(\n",
    "        signal=signals[signal_name],\n",
    "        target_change=cdx_spread,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    results_dict[signal_name] = result\n",
    "    \n",
    "    # Display decision\n",
    "    decision_indicator = {\n",
    "        \"PASS\": \"✅\",\n",
    "        \"HOLD\": \"⚠️\",\n",
    "        \"FAIL\": \"❌\",\n",
    "    }[result.decision]\n",
    "    \n",
    "    print(f\"  {decision_indicator} {result.decision}: Score = {result.composite_score:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"✓ Evaluated {len(results_dict)} signals\\n\")\n",
    "\n",
    "# Create evaluation summary table\n",
    "eval_summary_data = []\n",
    "for signal_name, result in results_dict.items():\n",
    "    decision_indicator = {\n",
    "        \"PASS\": \"✅ PASS\",\n",
    "        \"HOLD\": \"⚠️ HOLD\",\n",
    "        \"FAIL\": \"❌ FAIL\",\n",
    "    }[result.decision]\n",
    "    \n",
    "    eval_summary_data.append({\n",
    "        'Signal': signal_name,\n",
    "        'Decision': decision_indicator,\n",
    "        'Composite': f\"{result.composite_score:.3f}\",\n",
    "        'Data Health': f\"{result.data_health_score:.3f}\",\n",
    "        'Predictive': f\"{result.predictive_score:.3f}\",\n",
    "        'Economic': f\"{result.economic_score:.3f}\",\n",
    "        'Stability': f\"{result.stability_score:.3f}\",\n",
    "    })\n",
    "\n",
    "eval_summary_df = pd.DataFrame(eval_summary_data)\n",
    "\n",
    "print(f\"\\nEvaluation Summary:\\n\")\n",
    "print(eval_summary_df.to_markdown(index=False))\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37736f75",
   "metadata": {},
   "source": [
    "## 6. Visualize Composite Scores\n",
    "\n",
    "Plot composite scores with decision thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"VISUALIZING EVALUATION RESULTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = []\n",
    "for signal_name, result in results_dict.items():\n",
    "    plot_data.append({\n",
    "        'Signal': signal_name,\n",
    "        'Composite Score': result.composite_score,\n",
    "        'Decision': result.decision,\n",
    "    })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Create bar chart with discrete colors by decision\n",
    "color_map = {\n",
    "    'PASS': '#2ecc71',   # Green\n",
    "    'HOLD': '#f39c12',   # Yellow/Orange\n",
    "    'FAIL': '#e74c3c',   # Red\n",
    "}\n",
    "\n",
    "fig = px.bar(\n",
    "    plot_df,\n",
    "    x='Signal',\n",
    "    y='Composite Score',\n",
    "    color='Decision',\n",
    "    color_discrete_map=color_map,\n",
    "    title='Signal Suitability Composite Scores',\n",
    ")\n",
    "\n",
    "# Add threshold lines with annotations\n",
    "fig.add_hline(\n",
    "    y=0.7,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"gray\",\n",
    "    annotation_text=\"PASS threshold (≥0.7)\",\n",
    "    annotation_position=\"right\",\n",
    ")\n",
    "\n",
    "fig.add_hline(\n",
    "    y=0.4,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"gray\",\n",
    "    annotation_text=\"HOLD threshold (≥0.4)\",\n",
    "    annotation_position=\"right\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Signal\",\n",
    "    yaxis_title=\"Composite Score\",\n",
    "    template=\"plotly_white\",\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"✓ Composite score chart complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e818a7cc",
   "metadata": {},
   "source": [
    "## 7. Visualize Component Scores\n",
    "\n",
    "Heatmap showing breakdown of component scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract component scores into DataFrame\n",
    "component_data = []\n",
    "for signal_name, result in results_dict.items():\n",
    "    component_data.append({\n",
    "        'Signal': signal_name,\n",
    "        'Data Health': result.data_health_score,\n",
    "        'Predictive': result.predictive_score,\n",
    "        'Economic': result.economic_score,\n",
    "        'Stability': result.stability_score,\n",
    "    })\n",
    "\n",
    "component_df = pd.DataFrame(component_data)\n",
    "component_df = component_df.set_index('Signal')\n",
    "\n",
    "# Create heatmap\n",
    "fig = px.imshow(\n",
    "    component_df,\n",
    "    text_auto=\".3f\",\n",
    "    color_continuous_scale='RdYlGn',\n",
    "    zmin=0,\n",
    "    zmax=1,\n",
    "    title='Component Score Breakdown by Signal',\n",
    "    labels=dict(color=\"Score\"),\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Component\",\n",
    "    yaxis_title=\"Signal\",\n",
    "    template=\"plotly_white\",\n",
    "    height=400,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"✓ Component score heatmap complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00460cfa",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictive vs Economic Scores\n",
    "\n",
    "Scatter plot showing relationship between predictive and economic components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare scatter plot data\n",
    "scatter_data = []\n",
    "for signal_name, result in results_dict.items():\n",
    "    scatter_data.append({\n",
    "        'Signal': signal_name,\n",
    "        'Predictive Score': result.predictive_score,\n",
    "        'Economic Score': result.economic_score,\n",
    "        'Decision': result.decision,\n",
    "    })\n",
    "\n",
    "scatter_df = pd.DataFrame(scatter_data)\n",
    "\n",
    "# Create scatter plot\n",
    "fig = px.scatter(\n",
    "    scatter_df,\n",
    "    x='Predictive Score',\n",
    "    y='Economic Score',\n",
    "    color='Decision',\n",
    "    color_discrete_map=color_map,\n",
    "    text='Signal',\n",
    "    title='Predictive vs Economic Scores',\n",
    ")\n",
    "\n",
    "# Add reference lines at 0.5\n",
    "fig.add_hline(\n",
    "    y=0.5,\n",
    "    line_dash=\"dot\",\n",
    "    line_color=\"gray\",\n",
    "    opacity=0.5,\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=0.5,\n",
    "    line_dash=\"dot\",\n",
    "    line_color=\"gray\",\n",
    "    opacity=0.5,\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    textposition='top center',\n",
    "    marker=dict(size=12),\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Predictive Score\",\n",
    "    yaxis_title=\"Economic Score\",\n",
    "    template=\"plotly_white\",\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\n✓ Scatter plot complete\")\n",
    "print(f\"\\n✓ All visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeceaa83",
   "metadata": {},
   "source": [
    "## 9. Generate and Save Evaluation Reports\n",
    "\n",
    "Create Markdown reports for each signal evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"GENERATING EVALUATION REPORTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Reports directory (already created by ensure_directories)\n",
    "reports_dir = EVALUATION_DIR\n",
    "\n",
    "# Generate and save reports\n",
    "report_paths = {}\n",
    "\n",
    "for signal_name, result in results_dict.items():\n",
    "    print(f\"Generating report for {signal_name}...\")\n",
    "    \n",
    "    # Generate report content\n",
    "    report_content = generate_suitability_report(\n",
    "        result=result,\n",
    "        signal_id=signal_name,\n",
    "        product_id=\"CDX_IG_5Y\",\n",
    "    )\n",
    "    \n",
    "    # Save report (returns timestamped path)\n",
    "    report_path = save_report(\n",
    "        report=report_content,\n",
    "        signal_id=signal_name,\n",
    "        product_id=\"CDX_IG_5Y\",\n",
    "        output_dir=reports_dir,\n",
    "    )\n",
    "    \n",
    "    report_paths[signal_name] = report_path\n",
    "    print(f\"  ✓ Saved to: {report_path.name}\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(report_paths)} reports\")\n",
    "print(f\"\\nReport files:\")\n",
    "for signal_name, path in report_paths.items():\n",
    "    print(f\"  {path}\")\n",
    "\n",
    "print(f\"\\n✓ Reports saved to {reports_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73038430",
   "metadata": {},
   "source": [
    "## 10. Register Evaluations\n",
    "\n",
    "Track evaluation metadata in suitability registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"REGISTERING EVALUATIONS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Initialize registry\n",
    "registry = SuitabilityRegistry(SUITABILITY_REGISTRY_PATH)\n",
    "\n",
    "# Register each evaluation\n",
    "eval_ids = {}\n",
    "\n",
    "for signal_name, result in results_dict.items():\n",
    "    report_path = report_paths[signal_name]\n",
    "    \n",
    "    print(f\"Registering {signal_name}...\")\n",
    "    \n",
    "    eval_id = registry.register_evaluation(\n",
    "        result=result,\n",
    "        signal_id=signal_name,\n",
    "        product_id=\"CDX_IG_5Y\",\n",
    "        report_path=str(report_path),\n",
    "        evaluator_version=\"0.1.0\",\n",
    "    )\n",
    "    \n",
    "    eval_ids[signal_name] = eval_id\n",
    "    print(f\"  ✓ Registered as: {eval_id}\")\n",
    "\n",
    "print(f\"\\n✓ Registered {len(eval_ids)} evaluations\\n\")\n",
    "\n",
    "# Display registry summary\n",
    "total_evals = len(registry.list_evaluations())\n",
    "pass_evals = len(registry.list_evaluations(decision=\"PASS\"))\n",
    "hold_evals = len(registry.list_evaluations(decision=\"HOLD\"))\n",
    "fail_evals = len(registry.list_evaluations(decision=\"FAIL\"))\n",
    "\n",
    "print(f\"Registry Summary:\")\n",
    "print(f\"  Total evaluations: {total_evals}\")\n",
    "print(f\"  PASS: {pass_evals}\")\n",
    "print(f\"  HOLD: {hold_evals}\")\n",
    "print(f\"  FAIL: {fail_evals}\")\n",
    "\n",
    "print(f\"\\n✓ Registry updated at {SUITABILITY_REGISTRY_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6239ad",
   "metadata": {},
   "source": [
    "## 11. Persist Evaluation Metadata\n",
    "\n",
    "Save evaluation metadata for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d1908",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PERSISTING METADATA\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Count decisions\n",
    "pass_count = sum(1 for r in results_dict.values() if r.decision == \"PASS\")\n",
    "hold_count = sum(1 for r in results_dict.values() if r.decision == \"HOLD\")\n",
    "fail_count = sum(1 for r in results_dict.values() if r.decision == \"FAIL\")\n",
    "\n",
    "# Calculate mean composite score\n",
    "mean_score = sum(r.composite_score for r in results_dict.values()) / len(results_dict)\n",
    "\n",
    "# Build metadata\n",
    "metadata = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"lags\": config.lags,\n",
    "        \"min_obs\": config.min_obs,\n",
    "        \"pass_threshold\": config.pass_threshold,\n",
    "        \"hold_threshold\": config.hold_threshold,\n",
    "        \"data_health_weight\": config.data_health_weight,\n",
    "        \"predictive_weight\": config.predictive_weight,\n",
    "        \"economic_weight\": config.economic_weight,\n",
    "        \"stability_weight\": config.stability_weight,\n",
    "    },\n",
    "    \"signals_evaluated\": list(results_dict.keys()),\n",
    "    \"results_summary\": {\n",
    "        \"pass_count\": pass_count,\n",
    "        \"hold_count\": hold_count,\n",
    "        \"fail_count\": fail_count,\n",
    "        \"mean_composite_score\": mean_score,\n",
    "    },\n",
    "    \"report_directory\": str(EVALUATION_DIR),\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = LOGS_DIR / \"suitability_evaluation_metadata.json\"\n",
    "save_json(metadata, metadata_path)\n",
    "\n",
    "metadata_size_kb = metadata_path.stat().st_size / 1024\n",
    "\n",
    "print(f\"✓ Metadata saved to: {metadata_path}\")\n",
    "print(f\"  Size: {metadata_size_kb:.2f} KB\")\n",
    "\n",
    "print(f\"\\n✓ Metadata persisted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4ad7e",
   "metadata": {},
   "source": [
    "## 12. Decision Summary\n",
    "\n",
    "Determine which signals proceed to backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84577d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DECISION SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Filter signals by decision\n",
    "pass_signals = [name for name, result in results_dict.items() if result.decision == \"PASS\"]\n",
    "hold_signals = [name for name, result in results_dict.items() if result.decision == \"HOLD\"]\n",
    "fail_signals = [name for name, result in results_dict.items() if result.decision == \"FAIL\"]\n",
    "\n",
    "# Create summary table\n",
    "decision_summary = [\n",
    "    {\n",
    "        'Decision': '✅ PASS',\n",
    "        'Count': len(pass_signals),\n",
    "        'Signals': ', '.join(pass_signals) if pass_signals else '(none)',\n",
    "    },\n",
    "    {\n",
    "        'Decision': '⚠️ HOLD',\n",
    "        'Count': len(hold_signals),\n",
    "        'Signals': ', '.join(hold_signals) if hold_signals else '(none)',\n",
    "    },\n",
    "    {\n",
    "        'Decision': '❌ FAIL',\n",
    "        'Count': len(fail_signals),\n",
    "        'Signals': ', '.join(fail_signals) if fail_signals else '(none)',\n",
    "    },\n",
    "]\n",
    "\n",
    "decision_df = pd.DataFrame(decision_summary)\n",
    "print(decision_df.to_markdown(index=False))\n",
    "\n",
    "# Next steps guidance\n",
    "print(f\"\\n\\nNext Steps:\")\n",
    "\n",
    "if len(pass_signals) == 0:\n",
    "    print(f\"\\n⚠️  WARNING: No signals passed evaluation\")\n",
    "    print(f\"\\nReview component scores to identify weaknesses:\")\n",
    "    print(f\"  - Data health: Check sample size and missing data\")\n",
    "    print(f\"  - Predictive: Review correlations and t-statistics\")\n",
    "    print(f\"  - Economic: Verify effect size is meaningful\")\n",
    "    print(f\"  - Stability: Check sign consistency across subperiods\")\n",
    "    print(f\"\\nConsider signal refinements before proceeding to backtest.\")\n",
    "else:\n",
    "    print(f\"\\n✓ {len(pass_signals)} signal(s) passed evaluation\")\n",
    "    print(f\"\\nReady to proceed to Step 4 (Backtest Execution):\")\n",
    "    for signal in pass_signals:\n",
    "        print(f\"  - {signal}\")\n",
    "\n",
    "if len(hold_signals) > 0:\n",
    "    print(f\"\\n⚠️  {len(hold_signals)} signal(s) require manual review (HOLD decision)\")\n",
    "    print(f\"\\nHOLD signals flagged for judgment:\")\n",
    "    for signal in hold_signals:\n",
    "        score = results_dict[signal].composite_score\n",
    "        print(f\"  - {signal} (score: {score:.3f})\")\n",
    "\n",
    "if len(fail_signals) > 0:\n",
    "    print(f\"\\n❌ {len(fail_signals)} signal(s) failed evaluation\")\n",
    "    print(f\"\\nFAIL signals archived (do not backtest):\")\n",
    "    for signal in fail_signals:\n",
    "        score = results_dict[signal].composite_score\n",
    "        print(f\"  - {signal} (score: {score:.3f})\")\n",
    "\n",
    "print(f\"\\n✓ Decision summary complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4931650",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workflow Complete\n",
    "\n",
    "Signal suitability evaluation successful! Signals have been assessed using the four-component framework and decisions have been made.\n",
    "\n",
    "### What Was Accomplished\n",
    "\n",
    "✓ **Signals Loaded** — Imported computed signals from Step 2  \n",
    "✓ **Target Loaded** — Fetched CDX spread data for evaluation  \n",
    "✓ **Evaluations Complete** — Four-component scoring for all signals  \n",
    "✓ **Visualizations Created** — Composite scores, components, scatter plots  \n",
    "✓ **Reports Generated** — Markdown reports saved for each signal  \n",
    "✓ **Registry Updated** — Evaluation metadata tracked in catalog  \n",
    "✓ **Decisions Made** — Signals classified as PASS/HOLD/FAIL\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "```\n",
    "Signals DataFrame (Step 2)\n",
    "    ↓\n",
    "Suitability Evaluation (this notebook)\n",
    "├─ Four-component scoring\n",
    "├─ Decision thresholds applied\n",
    "└─ Registry tracking\n",
    "    ↓\n",
    "Filtered Signals\n",
    "├─ PASS → Backtest Execution (Step 4)\n",
    "├─ HOLD → Manual Review\n",
    "└─ FAIL → Archived\n",
    "```\n",
    "\n",
    "### Re-Running This Notebook\n",
    "\n",
    "- **Evaluation recomputation:** Scores are recalculated from scratch each run\n",
    "- **Reports:** New timestamped reports created (previous reports preserved)\n",
    "- **Registry:** New evaluations appended to registry catalog\n",
    "- **Metadata:** Overwrites previous `suitability_evaluation_metadata.json`\n",
    "- **Configuration changes:** Edit `min_obs` or config parameters in cell 4\n",
    "\n",
    "### Key Files Generated\n",
    "\n",
    "```\n",
    "reports/\n",
    "└── suitability/\n",
    "    ├── cdx_etf_basis_CDX_IG_5Y_{timestamp}.md\n",
    "    ├── cdx_vix_gap_CDX_IG_5Y_{timestamp}.md\n",
    "    └── spread_momentum_CDX_IG_5Y_{timestamp}.md\n",
    "\n",
    "logs/\n",
    "└── suitability_evaluation_metadata.json (updated)\n",
    "\n",
    "src/aponyx/evaluation/suitability/\n",
    "└── suitability_registry.json (updated)\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Signals file not found:**\n",
    "- Run `02_signal_computation.ipynb` first\n",
    "- Verify file exists: `data/processed/signals.parquet`\n",
    "- Check DATA_DIR configuration\n",
    "\n",
    "**Evaluation errors:**\n",
    "- Check signal and target alignment (DatetimeIndex)\n",
    "- Verify minimum observations threshold (252 by default)\n",
    "- Review ERROR logs for missing data or invalid inputs\n",
    "- Ensure signal has `.name` attribute\n",
    "\n",
    "**All signals FAIL:**\n",
    "- Review component scores in evaluation summary (cell 5)\n",
    "- Check data health: sufficient observations and low missing data?\n",
    "- Check predictive: are t-statistics significant (>2.0)?\n",
    "- Check economic: is effect size meaningful (>0.5 bps)?\n",
    "- Check stability: is sign consistent across subperiods?\n",
    "- Consider adjusting signal specifications or lookback periods\n",
    "\n",
    "**Low scores despite good signals:**\n",
    "- Review component weights in configuration (cell 4)\n",
    "- Check lag horizons match signal characteristics\n",
    "- Verify target product is appropriate for signal type\n",
    "- Consider adjusting min_obs threshold if sample is limited\n",
    "\n",
    "**Registry or report errors:**\n",
    "- Ensure EVALUATION_DIR and SUITABILITY_REGISTRY_PATH are configured\n",
    "- Check write permissions for reports/ and src/aponyx/evaluation/\n",
    "- Verify JSON registry is valid (not corrupted)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
