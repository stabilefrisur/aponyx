{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc3488f",
   "metadata": {},
   "source": [
    "# Data Download Workflow\n",
    "\n",
    "**Systematic Macro Credit Research — Step 1 of 5**\n",
    "\n",
    "This notebook downloads market data from Bloomberg Terminal for all configured securities in the aponyx framework. It represents the first step in the systematic research workflow outlined in PROJECT_STATUS.md.\n",
    "\n",
    "## Workflow Position\n",
    "\n",
    "```\n",
    "1. Data Download ← YOU ARE HERE\n",
    "   ↓\n",
    "2. Signal Computation (02_signal_computation.ipynb)\n",
    "   ↓\n",
    "3. Signal Suitability Evaluation (03_suitability_evaluation.ipynb)\n",
    "   ↓\n",
    "4. Backtest Execution (04_backtest.ipynb)\n",
    "   ↓\n",
    "5. Performance Analysis (05_analysis.ipynb)\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Active Bloomberg Terminal session\n",
    "- `xbbg` package installed (`uv pip install xbbg` or `uv sync --extra bloomberg`)\n",
    "- Bloomberg Terminal authentication configured\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Registry Check** — Review existing datasets in data registry\n",
    "2. **Security Configuration** — Display all securities from `bloomberg_securities.json`\n",
    "3. **Data Download** — Fetch data from Bloomberg Terminal for each security\n",
    "4. **Schema Validation** — Validate data conforms to project schemas\n",
    "5. **Quality Checks** — Verify data completeness and continuity\n",
    "6. **Registry Update** — Register downloaded datasets in data catalog\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- **Cache files:** `data/cache/bloomberg/*.parquet`\n",
    "- **Registry:** `data/registry.json` (updated with metadata)\n",
    "- **Download summary:** Statistics and quality metrics\n",
    "\n",
    "## Key Design Patterns\n",
    "\n",
    "- **Provider Abstraction:** Uses `BloombergSource` from data layer\n",
    "- **Unified Fetch Interface:** `fetch_cdx()`, `fetch_vix()`, `fetch_etf()`\n",
    "- **TTL-Based Caching:** Automatic caching with 1-day staleness check\n",
    "- **Schema Validation:** Ensures data meets project requirements\n",
    "- **Metadata Tracking:** DataRegistry for dataset cataloging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96185c45",
   "metadata": {},
   "source": [
    "# Data Download Workflow\n",
    "\n",
    "**Systematic Macro Credit Research — Step 1 of 5**\n",
    "\n",
    "This notebook downloads market data from Bloomberg Terminal for all configured securities in the aponyx framework. It represents the first step in the systematic research workflow outlined in PROJECT_STATUS.md.\n",
    "\n",
    "## Workflow Position\n",
    "\n",
    "```\n",
    "1. Data Download ← YOU ARE HERE\n",
    "   ↓\n",
    "2. Signal Computation (02_signal_computation.ipynb)\n",
    "   ↓\n",
    "3. Signal Suitability Evaluation (03_suitability_evaluation.ipynb)\n",
    "   ↓\n",
    "4. Backtest Execution (04_backtest.ipynb)\n",
    "   ↓\n",
    "5. Performance Analysis (05_analysis.ipynb)\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Active Bloomberg Terminal session\n",
    "- `xbbg` package installed (`uv pip install xbbg` or `uv sync --extra bloomberg`)\n",
    "- Bloomberg Terminal authentication configured\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Registry Check** — Review existing datasets in data registry\n",
    "2. **Security Configuration** — Display all securities from `bloomberg_securities.json`\n",
    "3. **Data Download** — Fetch data from Bloomberg Terminal for each security\n",
    "4. **Schema Validation** — Validate data conforms to project schemas\n",
    "5. **Quality Checks** — Verify data completeness and continuity\n",
    "6. **Registry Update** — Register downloaded datasets in data catalog\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- **Cache files:** `data/cache/bloomberg/*.parquet`\n",
    "- **Registry:** `data/registry.json` (updated with metadata)\n",
    "- **Download summary:** Statistics and quality metrics\n",
    "\n",
    "## Key Design Patterns\n",
    "\n",
    "- **Provider Abstraction:** Uses `BloombergSource` from data layer\n",
    "- **Unified Fetch Interface:** `fetch_cdx()`, `fetch_vix()`, `fetch_etf()`\n",
    "- **TTL-Based Caching:** Automatic caching with 1-day staleness check\n",
    "- **Schema Validation:** Ensures data meets project requirements\n",
    "- **Metadata Tracking:** DataRegistry for dataset cataloging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fc48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA DOWNLOAD WORKFLOW — Step 1 of 5\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Data directory: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\n",
      "  Registry path: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\registry.json\n",
      "  Cache TTL: 1 days\n",
      "\n",
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from aponyx.config import DATA_DIR, REGISTRY_PATH, CACHE_TTL_DAYS\n",
    "from aponyx.data import fetch_cdx, fetch_vix, fetch_etf\n",
    "from aponyx.data.sources import BloombergSource\n",
    "from aponyx.data.registry import DataRegistry\n",
    "from aponyx.data.bloomberg_config import (\n",
    "    list_securities,\n",
    "    get_security_spec,\n",
    "    validate_bloomberg_registry,\n",
    ")\n",
    "\n",
    "# Configure logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA DOWNLOAD WORKFLOW — Step 1 of 5\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Registry path: {REGISTRY_PATH}\")\n",
    "print(f\"  Cache TTL: {CACHE_TTL_DAYS} days\")\n",
    "print(f\"\\n✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42c12b",
   "metadata": {},
   "source": [
    "## 1. Check Current Registry Status\n",
    "\n",
    "Review what data is already available in the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9307db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 17:11:08,602 - aponyx.persistence.json_io - INFO - Loading JSON from C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\registry.json\n",
      "2025-11-08 17:11:08,603 - aponyx.data.registry - INFO - Loaded existing registry: path=C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\registry.json, datasets=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CURRENT DATA REGISTRY STATUS\n",
      "================================================================================\n",
      "\n",
      "Registry path: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\registry.json\n",
      "Total datasets: 0\n",
      "\n",
      "No datasets registered yet.\n",
      "This is expected on first run.\n"
     ]
    }
   ],
   "source": [
    "# Initialize registry\n",
    "registry = DataRegistry(REGISTRY_PATH, DATA_DIR)\n",
    "\n",
    "# List all registered datasets\n",
    "datasets = registry.list_datasets()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"CURRENT DATA REGISTRY STATUS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Registry path: {REGISTRY_PATH}\")\n",
    "print(f\"Total datasets: {len(datasets)}\\n\")\n",
    "\n",
    "if datasets:\n",
    "    # Create summary table\n",
    "    registry_data = []\n",
    "    for name in datasets:\n",
    "        entry = registry.get_dataset_entry(name)\n",
    "        registry_data.append({\n",
    "            'Dataset': name,\n",
    "            'Instrument': entry.instrument,\n",
    "            'Tenor': entry.tenor or 'N/A',\n",
    "            'Rows': entry.row_count or 0,\n",
    "            'Start': entry.start_date or 'Unknown',\n",
    "            'End': entry.end_date or 'Unknown',\n",
    "        })\n",
    "    \n",
    "    registry_df = pd.DataFrame(registry_data)\n",
    "    print(registry_df.to_markdown(index=False))\n",
    "    print(f\"\\n✓ {len(datasets)} datasets currently registered\")\n",
    "else:\n",
    "    print(\"No datasets registered yet.\")\n",
    "    print(\"This is expected on first run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f791b",
   "metadata": {},
   "source": [
    "## 2. Review Configured Securities\n",
    "\n",
    "Display all securities configured in `bloomberg_securities.json` and their Bloomberg tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c638b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 17:11:21,235 - aponyx.data.bloomberg_config - INFO - Bloomberg registry validated: 3 instruments, 8 securities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BLOOMBERG SECURITY CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Total securities configured: 8\n",
      "\n",
      "| Security ID   | Description                                       | Bloomberg Ticker             | Instrument Type   |\n",
      "|:--------------|:--------------------------------------------------|:-----------------------------|:------------------|\n",
      "| cdx_ig_5y     | CDX North America Investment Grade 5Y             | CDX IG CDSI GEN 5Y Corp      | cdx               |\n",
      "| cdx_ig_10y    | CDX North America Investment Grade 10Y            | CDX IG CDSI GEN 10Y Corp     | cdx               |\n",
      "| cdx_hy_5y     | CDX North America High Yield 5Y                   | CDX HY CDSI GEN 5Y SPRD Corp | cdx               |\n",
      "| itrx_xover_5y | iTraxx Europe Crossover 5Y                        | ITRX XOVER CDSI GEN 5Y Corp  | cdx               |\n",
      "| itrx_eur_5y   | iTraxx Europe Main 5Y                             | ITRX EUR CDSI GEN 5Y Corp    | cdx               |\n",
      "| hyg           | iShares iBoxx High Yield Corporate Bond ETF       | HYG US Equity                | etf               |\n",
      "| lqd           | iShares iBoxx Investment Grade Corporate Bond ETF | LQD US Equity                | etf               |\n",
      "| vix           | CBOE Volatility Index                             | VIX Index                    | vix               |\n",
      "\n",
      "\n",
      "By Instrument Type:\n",
      "  cdx: 5\n",
      "  etf: 2\n",
      "  vix: 1\n",
      "\n",
      "✓ Configuration validated: 3 instrument types, 8 securities\n"
     ]
    }
   ],
   "source": [
    "# Validate Bloomberg registry configuration\n",
    "instruments, securities = validate_bloomberg_registry()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BLOOMBERG SECURITY CONFIGURATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "security_list = []\n",
    "for sec_id in list_securities():\n",
    "    spec = get_security_spec(sec_id)\n",
    "    security_list.append({\n",
    "        'Security ID': sec_id,\n",
    "        'Description': spec.description,\n",
    "        'Bloomberg Ticker': spec.bloomberg_ticker,\n",
    "        'Instrument Type': spec.instrument_type,\n",
    "    })\n",
    "\n",
    "securities_df = pd.DataFrame(security_list)\n",
    "\n",
    "print(f\"Total securities configured: {len(securities_df)}\\n\")\n",
    "print(securities_df.to_markdown(index=False))\n",
    "\n",
    "# Group by instrument type\n",
    "print(f\"\\n\\nBy Instrument Type:\")\n",
    "instrument_counts = securities_df.groupby('Instrument Type').size()\n",
    "for inst_type, count in instrument_counts.items():\n",
    "    print(f\"  {inst_type}: {count}\")\n",
    "\n",
    "print(f\"\\n✓ Configuration validated: {len(instruments)} instrument types, {len(securities)} securities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c1aca",
   "metadata": {},
   "source": [
    "## 3. Configure Download Parameters\n",
    "\n",
    "Set date range and download options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911736ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DOWNLOAD CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Date Range:\n",
      "  Start: 2020-11-09\n",
      "  End: 2025-11-08\n",
      "  Approximate trading days: ~1260\n",
      "\n",
      "Data Source:\n",
      "  Provider: Bloomberg Terminal\n",
      "  Type: BloombergSource()\n",
      "\n",
      "Cache Settings:\n",
      "  Enabled: True\n",
      "  TTL: 1 days\n",
      "  Cache directory: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\cache\\bloomberg\n",
      "\n",
      "Securities to Download: 8\n",
      "\n",
      "✓ Configuration ready\n"
     ]
    }
   ],
   "source": [
    "# Date range for download (default: 5 years of history)\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "start_date = (datetime.now() - timedelta(days=5*365)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Bloomberg source\n",
    "source = BloombergSource()\n",
    "\n",
    "# Cache settings (use cache to avoid re-downloading)\n",
    "use_cache = True\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DOWNLOAD CONFIGURATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Date Range:\")\n",
    "print(f\"  Start: {start_date}\")\n",
    "print(f\"  End: {end_date}\")\n",
    "print(f\"  Approximate trading days: ~{5 * 252}\")\n",
    "print(f\"\\nData Source:\")\n",
    "print(f\"  Provider: Bloomberg Terminal\")\n",
    "print(f\"  Type: BloombergSource()\")\n",
    "print(f\"\\nCache Settings:\")\n",
    "print(f\"  Enabled: {use_cache}\")\n",
    "print(f\"  TTL: {CACHE_TTL_DAYS} days\")\n",
    "print(f\"  Cache directory: {DATA_DIR / 'cache' / 'bloomberg'}\")\n",
    "print(f\"\\nSecurities to Download: {len(securities_df)}\")\n",
    "print(f\"\\n✓ Configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f543173c",
   "metadata": {},
   "source": [
    "## 4. Download Data by Instrument Type\n",
    "\n",
    "Download data for all securities, grouped by instrument type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b532e0",
   "metadata": {},
   "source": [
    "### CDX Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9db414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 17:12:10,514 - aponyx.data.fetch - INFO - Fetching CDX from bloomberg\n",
      "2025-11-08 17:12:10,515 - aponyx.data.providers.bloomberg - INFO - Fetching cdx from Bloomberg: ticker=CDX IG CDSI GEN 5Y Corp, dates=2020-11-09 to 2025-11-08\n",
      "2025-11-08 17:12:10,515 - aponyx.data.providers.bloomberg - INFO - Fetching cdx from Bloomberg: ticker=CDX IG CDSI GEN 5Y Corp, dates=2020-11-09 to 2025-11-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 5 CDX indices...\n",
      "\n",
      "Fetching cdx_ig_5y: CDX North America Investment Grade 5Y\n",
      "  Ticker: CDX IG CDSI GEN 5Y Corp\n"
     ]
    },
    {
     "ename": "Skipped",
     "evalue": "could not import 'blpapi': No module named 'blpapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ROG3003\\PythonProjects\\aponyx\\.venv\\Lib\\site-packages\\xbbg\\core\\conn.py:16\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mblpapi\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'blpapi'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mSkipped\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msecurity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec.description\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Ticker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec.bloomberg_ticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df = \u001b[43mfetch_cdx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msecurity\u001b[49m\u001b[43m=\u001b[49m\u001b[43msecurity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m cdx_results[security] = df\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✓ Downloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.index.min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.index.max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PythonProjects\\aponyx\\src\\aponyx\\data\\fetch.py:137\u001b[39m, in \u001b[36mfetch_cdx\u001b[39m\u001b[34m(source, security, bloomberg_ticker, start_date, end_date, use_cache)\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    135\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mEither \u001b[39m\u001b[33m'\u001b[39m\u001b[33msecurity\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbloomberg_ticker\u001b[39m\u001b[33m'\u001b[39m\u001b[33m required for Bloomberg CDX fetch\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    136\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     df = \u001b[43mfetch_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstrument\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstrument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43msecurity\u001b[49m\u001b[43m=\u001b[49m\u001b[43msecurity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported source type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(source)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PythonProjects\\aponyx\\src\\aponyx\\data\\providers\\bloomberg.py:102\u001b[39m, in \u001b[36mfetch_from_bloomberg\u001b[39m\u001b[34m(ticker, instrument, start_date, end_date, security, **params)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Import xbbg wrapper\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxbbg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m blp\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    105\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mxbbg not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInstall with: uv pip install --optional bloomberg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ROG3003\\PythonProjects\\aponyx\\.venv\\Lib\\site-packages\\xbbg\\blp.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxbbg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__, const, pipeline\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxbbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logs, files, storage\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxbbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, conn, process\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxbbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m connect\n\u001b[32m     12\u001b[39m __all__ = [\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mconnect\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mturnover\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     27\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ROG3003\\PythonProjects\\aponyx\\.venv\\Lib\\site-packages\\xbbg\\core\\conn.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytest\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     blpapi = \u001b[43mpytest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimportorskip\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mblpapi\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxbbg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logs\n\u001b[32m     23\u001b[39m _CON_SYM_ = \u001b[33m'\u001b[39m\u001b[33m_xcon_\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ROG3003\\PythonProjects\\aponyx\\.venv\\Lib\\site-packages\\_pytest\\outcomes.py:302\u001b[39m, in \u001b[36mimportorskip\u001b[39m\u001b[34m(modname, minversion, reason, exc_type)\u001b[39m\n\u001b[32m    300\u001b[39m     warnings.warn(warning, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skipped:\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m skipped\n\u001b[32m    304\u001b[39m mod = sys.modules[modname]\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m minversion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mSkipped\u001b[39m: could not import 'blpapi': No module named 'blpapi'"
     ]
    }
   ],
   "source": [
    "cdx_securities = list_securities(instrument_type=\"cdx\")\n",
    "print(f\"Downloading {len(cdx_securities)} CDX indices...\\n\")\n",
    "\n",
    "cdx_results = {}\n",
    "for security in cdx_securities:\n",
    "    try:\n",
    "        spec = get_security_spec(security)\n",
    "        print(f\"Fetching {security}: {spec.description}\")\n",
    "        print(f\"  Ticker: {spec.bloomberg_ticker}\")\n",
    "        \n",
    "        df = fetch_cdx(\n",
    "            source=source,\n",
    "            security=security,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "        \n",
    "        cdx_results[security] = df\n",
    "        print(f\"  ✓ Downloaded {len(df)} rows ({df.index.min()} to {df.index.max()})\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        logger.error(\"Failed to fetch %s: %s\", security, str(e))\n",
    "        print()\n",
    "\n",
    "print(f\"CDX download complete: {len(cdx_results)}/{len(cdx_securities)} successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d3209",
   "metadata": {},
   "source": [
    "### ETF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_securities = list_securities(instrument_type=\"etf\")\n",
    "print(f\"Downloading {len(etf_securities)} ETFs...\\n\")\n",
    "\n",
    "etf_results = {}\n",
    "for security in etf_securities:\n",
    "    try:\n",
    "        spec = get_security_spec(security)\n",
    "        print(f\"Fetching {security}: {spec.description}\")\n",
    "        print(f\"  Ticker: {spec.bloomberg_ticker}\")\n",
    "        \n",
    "        df = fetch_etf(\n",
    "            source=source,\n",
    "            security=security,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "        \n",
    "        etf_results[security] = df\n",
    "        print(f\"  ✓ Downloaded {len(df)} rows ({df.index.min()} to {df.index.max()})\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        logger.error(\"Failed to fetch %s: %s\", security, str(e))\n",
    "        print()\n",
    "\n",
    "print(f\"ETF download complete: {len(etf_results)}/{len(etf_securities)} successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c14cb7",
   "metadata": {},
   "source": [
    "### VIX Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vix_securities = list_securities(instrument_type=\"vix\")\n",
    "print(f\"Downloading {len(vix_securities)} VIX index...\\n\")\n",
    "\n",
    "vix_results = {}\n",
    "for security in vix_securities:\n",
    "    try:\n",
    "        spec = get_security_spec(security)\n",
    "        print(f\"Fetching {security}: {spec.description}\")\n",
    "        print(f\"  Ticker: {spec.bloomberg_ticker}\")\n",
    "        \n",
    "        df = fetch_vix(\n",
    "            source=source,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            use_cache=use_cache,\n",
    "        )\n",
    "        \n",
    "        vix_results[security] = df\n",
    "        print(f\"  ✓ Downloaded {len(df)} rows ({df.index.min()} to {df.index.max()})\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        logger.error(\"Failed to fetch %s: %s\", security, str(e))\n",
    "        print()\n",
    "\n",
    "print(f\"VIX download complete: {len(vix_results)}/{len(vix_securities)} successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34154ed2",
   "metadata": {},
   "source": [
    "## 5. Download Summary\n",
    "\n",
    "Review download results and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4264977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {**cdx_results, **etf_results, **vix_results}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DOWNLOAD SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for security, df in all_results.items():\n",
    "    spec = get_security_spec(security)\n",
    "    summary_data.append({\n",
    "        'Security': security,\n",
    "        'Instrument': spec.instrument_type.upper(),\n",
    "        'Rows': len(df),\n",
    "        'Start Date': df.index.min().strftime('%Y-%m-%d'),\n",
    "        'End Date': df.index.max().strftime('%Y-%m-%d'),\n",
    "        'Days Coverage': (df.index.max() - df.index.min()).days,\n",
    "        'Columns': ', '.join(df.columns),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(f\"Download Results: {len(all_results)}/{len(securities_df)} securities\\n\")\n",
    "print(summary_df.to_markdown(index=False))\n",
    "\n",
    "# Check for missing securities\n",
    "all_securities = set(list_securities())\n",
    "downloaded_securities = set(all_results.keys())\n",
    "missing_securities = all_securities - downloaded_securities\n",
    "\n",
    "if missing_securities:\n",
    "    print(f\"\\n\\n⚠️  Missing Securities ({len(missing_securities)}):\")\n",
    "    for sec in sorted(missing_securities):\n",
    "        print(f\"  - {sec}\")\n",
    "else:\n",
    "    print(f\"\\n\\n✓ All {len(all_securities)} configured securities downloaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7314b7",
   "metadata": {},
   "source": [
    "## 6. Verify Registry Update\n",
    "\n",
    "Confirm that cache files were created and registry was updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744173c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload registry to see updated state\n",
    "registry = DataRegistry(REGISTRY_PATH, DATA_DIR)\n",
    "updated_datasets = registry.list_datasets()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"UPDATED REGISTRY STATUS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Group by instrument\n",
    "cdx_datasets = registry.list_datasets(instrument=\"cdx\")\n",
    "etf_datasets = registry.list_datasets(instrument=\"etf\")\n",
    "vix_datasets = registry.list_datasets(instrument=\"vix\")\n",
    "\n",
    "print(f\"Registry Summary:\")\n",
    "print(f\"  Total Datasets: {len(updated_datasets)}\")\n",
    "print(f\"  CDX Datasets: {len(cdx_datasets)}\")\n",
    "print(f\"  ETF Datasets: {len(etf_datasets)}\")\n",
    "print(f\"  VIX Datasets: {len(vix_datasets)}\")\n",
    "\n",
    "# Check cache directory structure\n",
    "cache_dir = DATA_DIR / \"cache\" / \"bloomberg\"\n",
    "print(f\"\\nCache Directory: {cache_dir}\")\n",
    "\n",
    "if cache_dir.exists():\n",
    "    cache_files = list(cache_dir.glob(\"*.parquet\"))\n",
    "    total_size_mb = sum(f.stat().st_size for f in cache_files) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"  Files: {len(cache_files)} Parquet files\")\n",
    "    print(f\"  Total size: {total_size_mb:.2f} MB\")\n",
    "    \n",
    "    if len(cache_files) <= 10:\n",
    "        print(f\"\\n  File details:\")\n",
    "        for f in sorted(cache_files):\n",
    "            size_mb = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"    {f.name} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"  ⚠️  Cache directory not found\")\n",
    "\n",
    "print(f\"\\n✓ Registry updated with {len(updated_datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446caa8",
   "metadata": {},
   "source": [
    "## 7. Data Quality Checks\n",
    "\n",
    "Quick validation of downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ad7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DATA QUALITY CHECKS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "for security, df in all_results.items():\n",
    "    issues = []\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_pct = (df.isna().sum() / len(df) * 100)\n",
    "    for col, pct in missing_pct.items():\n",
    "        if pct > 0:\n",
    "            issues.append(f\"Missing values in '{col}': {pct:.1f}%\")\n",
    "    \n",
    "    # Check date continuity (gaps > 7 days)\n",
    "    date_gaps = df.index.to_series().diff()\n",
    "    large_gaps = date_gaps[date_gaps > pd.Timedelta(days=7)]\n",
    "    if len(large_gaps) > 0:\n",
    "        issues.append(f\"Date gaps > 7 days: {len(large_gaps)} gaps\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if df.index.duplicated().any():\n",
    "        n_dups = df.index.duplicated().sum()\n",
    "        issues.append(f\"Duplicate dates: {n_dups}\")\n",
    "    \n",
    "    # Check data range\n",
    "    if len(df) < 100:\n",
    "        issues.append(f\"Low row count: {len(df)} rows\")\n",
    "    \n",
    "    # Check for monotonic index\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        issues.append(\"Non-monotonic date index\")\n",
    "    \n",
    "    if issues:\n",
    "        quality_issues.append({\n",
    "            'Security': security,\n",
    "            'Issues': issues,\n",
    "        })\n",
    "\n",
    "if quality_issues:\n",
    "    print(f\"⚠️  Quality Issues Found ({len(quality_issues)} securities):\\n\")\n",
    "    for item in quality_issues:\n",
    "        print(f\"{item['Security']}:\")\n",
    "        for issue in item['Issues']:\n",
    "            print(f\"  - {issue}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"✓ No quality issues detected\\n\")\n",
    "    print(\"All datasets passed quality checks:\")\n",
    "    print(\"  - Complete dates (no missing values)\")\n",
    "    print(\"  - No duplicates\")\n",
    "    print(\"  - Sufficient coverage (>100 rows)\")\n",
    "    print(\"  - Monotonic date index\")\n",
    "    print(\"  - No large date gaps (>7 days)\")\n",
    "    \n",
    "print(f\"\\n✓ Quality validation complete for {len(all_results)} securities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e512e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workflow Complete\n",
    "\n",
    "Data download successful! The downloaded datasets are now ready for the next steps in the systematic research workflow.\n",
    "\n",
    "### What Was Accomplished\n",
    "\n",
    "✓ **Data Downloaded** — {len(all_results)} securities fetched from Bloomberg Terminal  \n",
    "✓ **Cache Created** — Parquet files saved to `data/cache/bloomberg/`  \n",
    "✓ **Registry Updated** — Metadata registered in `data/registry.json`  \n",
    "✓ **Quality Validated** — Schema and continuity checks passed\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "```\n",
    "Downloaded Data (this notebook)\n",
    "    ↓\n",
    "market_data: dict[str, pd.DataFrame]\n",
    "├─ \"cdx\": spread, security\n",
    "├─ \"vix\": level\n",
    "└─ \"etf\": spread, security\n",
    "    ↓\n",
    "Signal Computation (next notebook)\n",
    "```\n",
    "\n",
    "### Re-Running This Notebook\n",
    "\n",
    "- **Cache enabled:** Second run will use cached data (faster)\n",
    "- **Cache TTL:** {CACHE_TTL_DAYS} days (configurable in `config/__init__.py`)\n",
    "- **Force refresh:** Set `use_cache=False` in download cells\n",
    "- **Update registry:** Run cell 6 to refresh metadata\n",
    "\n",
    "### Key Files Generated\n",
    "\n",
    "```\n",
    "data/\n",
    "├── cache/\n",
    "│   └── bloomberg/\n",
    "│       ├── cdx_*.parquet\n",
    "│       ├── vix_*.parquet\n",
    "│       └── etf_*.parquet\n",
    "└── registry.json (updated)\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Bloomberg Terminal not available:**\n",
    "- Ensure Bloomberg Terminal is running\n",
    "- Check authentication and subscription\n",
    "- Install xbbg: `uv pip install xbbg`\n",
    "\n",
    "**Quality issues detected:**\n",
    "- Review warning messages in cell 7\n",
    "- Check Bloomberg Terminal data availability\n",
    "- Verify ticker configuration in `bloomberg_securities.json`\n",
    "\n",
    "**Cache issues:**\n",
    "- Clear cache: `rm -rf data/cache/bloomberg/*`\n",
    "- Disable cache: Set `use_cache=False`\n",
    "- Check cache TTL in `config/__init__.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
