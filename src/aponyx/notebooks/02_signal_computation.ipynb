{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215de2db",
   "metadata": {},
   "source": [
    "# Signal Computation Workflow\n",
    "\n",
    "**Systematic Macro Credit Research — Step 2 of 5**\n",
    "\n",
    "This notebook computes tactical credit signals from cached market data. It represents the second step in the systematic research workflow outlined in PROJECT_STATUS.md.\n",
    "\n",
    "## Workflow Position\n",
    "\n",
    "```\n",
    "1. Data Download (01_data_download.ipynb)\n",
    "   ↓\n",
    "2. Signal Computation ← YOU ARE HERE\n",
    "   ↓\n",
    "3. Signal Suitability Evaluation (03_suitability_evaluation.ipynb)\n",
    "   ↓\n",
    "4. Backtest Execution (04_backtest.ipynb)\n",
    "   ↓\n",
    "5. Performance Analysis (05_analysis.ipynb)\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `01_data_download.ipynb` with cached market data\n",
    "- Cache files exist in `data/cache/bloomberg/`\n",
    "- Visualization dependencies installed (`uv sync --extra viz`)\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Load Market Data** — Read cached CDX, VIX, and ETF data from Step 1\n",
    "2. **Display Signal Catalog** — Show all registered signals and their requirements\n",
    "3. **Compute Signals** — Execute enabled signals via SignalRegistry\n",
    "4. **Validate Outputs** — Check z-score properties and alignment\n",
    "5. **Visualize Signals** — Plot time series and correlations\n",
    "6. **Persist Results** — Save signals DataFrame and metadata\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- **Signals DataFrame:** `data/processed/signals.parquet`\n",
    "- **Computation Metadata:** `logs/signal_computation_metadata.json`\n",
    "\n",
    "## Key Design Patterns\n",
    "\n",
    "- **SignalRegistry:** Batch computation of catalog-defined signals\n",
    "- **Z-Score Normalization:** All signals normalized for regime independence\n",
    "- **Sign Convention:** Positive values = long credit risk (buy CDX)\n",
    "- **Fail-Fast Validation:** Stop on missing data or configuration errors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37adf37a",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "Import dependencies and verify cache availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b95ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SIGNAL COMPUTATION WORKFLOW — Step 2 of 5\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Data directory: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\n",
      "  Logs directory: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\logs\n",
      "  Signal catalog: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\src\\aponyx\\models\\signal_catalog.json\n",
      "\n",
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from aponyx.config import DATA_DIR, LOGS_DIR, SIGNAL_CATALOG_PATH, REGISTRY_PATH\n",
    "from aponyx.data import fetch_cdx, fetch_vix, fetch_etf\n",
    "from aponyx.data.sources import BloombergSource\n",
    "from aponyx.data.registry import DataRegistry\n",
    "from aponyx.persistence import save_parquet, save_json\n",
    "from aponyx.models import compute_registered_signals\n",
    "from aponyx.models.registry import SignalRegistry\n",
    "from aponyx.models.config import SignalConfig\n",
    "from aponyx.visualization import plot_signal\n",
    "\n",
    "# Configure logging for notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SIGNAL COMPUTATION WORKFLOW — Step 2 of 5\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Logs directory: {LOGS_DIR}\")\n",
    "print(f\"  Signal catalog: {SIGNAL_CATALOG_PATH}\")\n",
    "print(f\"  Registry path: {REGISTRY_PATH}\")\n",
    "print(f\"\\n✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 19:24:00,768 - aponyx.persistence.parquet_io - INFO - Found 0 Parquet files in C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\cache\\bloomberg (pattern=*.parquet)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CACHE DIRECTORY STATUS\n",
      "================================================================================\n",
      "\n",
      "Cache directory: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\cache\\bloomberg\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No cache files found in C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\cache\\bloomberg\nRun 01_data_download.ipynb first to download market data.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m cache_files = list_parquet_files(cache_dir)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_files:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     20\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo cache files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRun 01_data_download.ipynb first to download market data.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAvailable cache files (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cache_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(cache_files):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No cache files found in C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\cache\\bloomberg\nRun 01_data_download.ipynb first to download market data."
     ]
    }
   ],
   "source": [
    "# Verify registry and cache availability\n",
    "registry = DataRegistry(REGISTRY_PATH, DATA_DIR)\n",
    "datasets = registry.list_datasets()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DATA REGISTRY STATUS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Registry path: {REGISTRY_PATH}\")\n",
    "print(f\"Total datasets: {len(datasets)}\")\n",
    "\n",
    "if not datasets:\n",
    "    raise FileNotFoundError(\n",
    "        \"No datasets found in registry.\\n\"\n",
    "        \"Run 01_data_download.ipynb first to download market data.\"\n",
    "    )\n",
    "\n",
    "# Display registered datasets by instrument\n",
    "cdx_datasets = registry.list_datasets(instrument=\"cdx\")\n",
    "vix_datasets = registry.list_datasets(instrument=\"vix\")\n",
    "etf_datasets = registry.list_datasets(instrument=\"etf\")\n",
    "\n",
    "print(f\"\\nBy Instrument Type:\")\n",
    "print(f\"  CDX: {len(cdx_datasets)} datasets\")\n",
    "print(f\"  VIX: {len(vix_datasets)} datasets\")\n",
    "print(f\"  ETF: {len(etf_datasets)} datasets\")\n",
    "\n",
    "print(f\"\\n✓ Registry verified with {len(datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2959cf0",
   "metadata": {},
   "source": [
    "## 2. Load Cached Market Data\n",
    "\n",
    "Load data using fetch functions which automatically handle cache lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85121520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING MARKET DATA\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "CDX cache file not found: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\cache\\bloomberg\\cdx_cdx_ig_5y.parquet\nRun 01_data_download.ipynb to download CDX IG 5Y data.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      7\u001b[39m cdx_path = cache_dir / \u001b[33m\"\u001b[39m\u001b[33mcdx_cdx_ig_5y.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m cdx_df = \u001b[43mload_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdx_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Loaded CDX IG 5Y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cdx_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PythonProjects\\aponyx\\src\\aponyx\\persistence\\parquet_io.py:116\u001b[39m, in \u001b[36mload_parquet\u001b[39m\u001b[34m(path, columns, start_date, end_date)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParquet file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    118\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mLoading Parquet file: path=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, columns=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, path, columns \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Parquet file not found: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\cache\\bloomberg\\cdx_cdx_ig_5y.parquet",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Date range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcdx_df.index.min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcdx_df.index.max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     14\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCDX cache file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcdx_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRun 01_data_download.ipynb to download CDX IG 5Y data.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Verify CDX has required column\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mspread\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cdx_df.columns:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: CDX cache file not found: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\data\\cache\\bloomberg\\cdx_cdx_ig_5y.parquet\nRun 01_data_download.ipynb to download CDX IG 5Y data."
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"LOADING MARKET DATA\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Initialize Bloomberg source (will use cache)\n",
    "source = BloombergSource()\n",
    "use_cache = True\n",
    "\n",
    "# Load CDX IG 5Y data\n",
    "print(\"Loading CDX IG 5Y...\")\n",
    "cdx_df = fetch_cdx(\n",
    "    source=source,\n",
    "    security=\"cdx_ig_5y\",\n",
    "    use_cache=use_cache,\n",
    ")\n",
    "print(f\"✓ Loaded CDX IG 5Y: {len(cdx_df)} rows\")\n",
    "print(f\"  Columns: {list(cdx_df.columns)}\")\n",
    "print(f\"  Date range: {cdx_df.index.min()} to {cdx_df.index.max()}\")\n",
    "\n",
    "# Verify CDX has required column\n",
    "if 'spread' not in cdx_df.columns:\n",
    "    raise ValueError(f\"CDX data missing 'spread' column. Found: {list(cdx_df.columns)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Load VIX data\n",
    "print(\"Loading VIX...\")\n",
    "vix_df = fetch_vix(\n",
    "    source=source,\n",
    "    use_cache=use_cache,\n",
    ")\n",
    "print(f\"✓ Loaded VIX: {len(vix_df)} rows\")\n",
    "print(f\"  Columns: {list(vix_df.columns)}\")\n",
    "print(f\"  Date range: {vix_df.index.min()} to {vix_df.index.max()}\")\n",
    "\n",
    "# Verify VIX has required column\n",
    "if 'close' not in vix_df.columns:\n",
    "    raise ValueError(f\"VIX data missing 'close' column. Found: {list(vix_df.columns)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Load ETF (HYG) data\n",
    "print(\"Loading HYG ETF...\")\n",
    "etf_df = fetch_etf(\n",
    "    source=source,\n",
    "    security=\"hyg\",\n",
    "    use_cache=use_cache,\n",
    ")\n",
    "print(f\"✓ Loaded HYG ETF: {len(etf_df)} rows\")\n",
    "print(f\"  Columns: {list(etf_df.columns)}\")\n",
    "print(f\"  Date range: {etf_df.index.min()} to {etf_df.index.max()}\")\n",
    "\n",
    "# Verify ETF has required column\n",
    "if 'close' not in etf_df.columns:\n",
    "    raise ValueError(f\"ETF data missing 'close' column. Found: {list(etf_df.columns)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Create market data dictionary\n",
    "market_data = {\n",
    "    \"cdx\": cdx_df,\n",
    "    \"vix\": vix_df,\n",
    "    \"etf\": etf_df,\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "summary_data = [\n",
    "    {\n",
    "        'Dataset': 'CDX IG 5Y',\n",
    "        'Rows': len(cdx_df),\n",
    "        'Start': cdx_df.index.min().strftime('%Y-%m-%d'),\n",
    "        'End': cdx_df.index.max().strftime('%Y-%m-%d'),\n",
    "        'Columns': ', '.join(cdx_df.columns),\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'VIX',\n",
    "        'Rows': len(vix_df),\n",
    "        'Start': vix_df.index.min().strftime('%Y-%m-%d'),\n",
    "        'End': vix_df.index.max().strftime('%Y-%m-%d'),\n",
    "        'Columns': ', '.join(vix_df.columns),\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'HYG ETF',\n",
    "        'Rows': len(etf_df),\n",
    "        'Start': etf_df.index.min().strftime('%Y-%m-%d'),\n",
    "        'End': etf_df.index.max().strftime('%Y-%m-%d'),\n",
    "        'Columns': ', '.join(etf_df.columns),\n",
    "    },\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(f\"\\nMarket Data Summary:\\n\")\n",
    "print(summary_df.to_markdown(index=False))\n",
    "print(f\"\\n✓ All market data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30a5f5",
   "metadata": {},
   "source": [
    "## 3. Display Signal Catalog\n",
    "\n",
    "Review all registered signals and identify which are enabled for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95631904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 18:14:03,085 - aponyx.models.registry - INFO - Loaded signal registry: catalog=C:\\Users\\ROG3003\\PythonProjects\\aponyx\\src\\aponyx\\models\\signal_catalog.json, signals=3, enabled=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SIGNAL CATALOG\n",
      "================================================================================\n",
      "\n",
      "Catalog path: C:\\Users\\ROG3003\\PythonProjects\\aponyx\\src\\aponyx\\models\\signal_catalog.json\n",
      "Total signals: 3\n",
      "Enabled signals: 3\n",
      "\n",
      "\n",
      "All Registered Signals:\n",
      "\n",
      "| Signal          | Description                                                                | Data Requirements      | Enabled   |\n",
      "|:----------------|:---------------------------------------------------------------------------|:-----------------------|:----------|\n",
      "| cdx_etf_basis   | Flow-driven mispricing signal from CDX-ETF basis divergence                | cdx:spread, etf:spread | ✓         |\n",
      "| cdx_vix_gap     | Cross-asset risk sentiment divergence between credit and equity volatility | cdx:spread, vix:level  | ✓         |\n",
      "| spread_momentum | Short-term volatility-adjusted momentum in CDX spreads                     | cdx:spread             | ✓         |\n",
      "\n",
      "\n",
      "Enabled Signals for Computation:\n",
      "\n",
      "| Signal          | Description                                                                | Function                |\n",
      "|:----------------|:---------------------------------------------------------------------------|:------------------------|\n",
      "| cdx_etf_basis   | Flow-driven mispricing signal from CDX-ETF basis divergence                | compute_cdx_etf_basis   |\n",
      "| cdx_vix_gap     | Cross-asset risk sentiment divergence between credit and equity volatility | compute_cdx_vix_gap     |\n",
      "| spread_momentum | Short-term volatility-adjusted momentum in CDX spreads                     | compute_spread_momentum |\n",
      "\n",
      "✓ Signal catalog loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize signal registry\n",
    "registry = SignalRegistry(SIGNAL_CATALOG_PATH)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SIGNAL CATALOG\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Catalog path: {SIGNAL_CATALOG_PATH}\")\n",
    "\n",
    "# Get all signals\n",
    "all_signals = registry.list_all()\n",
    "enabled_signals = registry.get_enabled()\n",
    "\n",
    "print(f\"Total signals: {len(all_signals)}\")\n",
    "print(f\"Enabled signals: {len(enabled_signals)}\")\n",
    "\n",
    "# Display all signals\n",
    "print(f\"\\n\\nAll Registered Signals:\\n\")\n",
    "catalog_data = []\n",
    "for name, metadata in all_signals.items():\n",
    "    data_req_str = ', '.join([f\"{k}:{v}\" for k, v in metadata.data_requirements.items()])\n",
    "    catalog_data.append({\n",
    "        'Signal': name,\n",
    "        'Description': metadata.description,\n",
    "        'Data Requirements': data_req_str,\n",
    "        'Enabled': '✓' if metadata.enabled else '✗',\n",
    "    })\n",
    "\n",
    "catalog_df = pd.DataFrame(catalog_data)\n",
    "print(catalog_df.to_markdown(index=False))\n",
    "\n",
    "# Display enabled signals separately\n",
    "if enabled_signals:\n",
    "    print(f\"\\n\\nEnabled Signals for Computation:\\n\")\n",
    "    enabled_data = []\n",
    "    for name, metadata in enabled_signals.items():\n",
    "        enabled_data.append({\n",
    "            'Signal': name,\n",
    "            'Description': metadata.description,\n",
    "            'Function': metadata.compute_function_name,\n",
    "        })\n",
    "    enabled_df = pd.DataFrame(enabled_data)\n",
    "    print(enabled_df.to_markdown(index=False))\n",
    "else:\n",
    "    print(\"\\n⚠️  No signals enabled in catalog\")\n",
    "\n",
    "print(f\"\\n✓ Signal catalog loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61492404",
   "metadata": {},
   "source": [
    "## 4. Configure Signal Computation\n",
    "\n",
    "Set parameters for signal calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb28bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SIGNAL CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Lookback window: 20 days\n",
      "  Minimum periods: 10 observations\n",
      "\n",
      "Rationale:\n",
      "  - 20-day lookback captures tactical credit signal dynamics\n",
      "  - Minimum 10 observations ensures statistical validity\n",
      "  - Z-score normalization for regime independence\n",
      "\n",
      "✓ Configuration ready\n"
     ]
    }
   ],
   "source": [
    "# Signal configuration\n",
    "config = SignalConfig(\n",
    "    lookback=20,      # 20-day rolling window for tactical signals\n",
    "    min_periods=10,   # Minimum 10 observations for statistical validity\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SIGNAL CONFIGURATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Lookback window: {config.lookback} days\")\n",
    "print(f\"  Minimum periods: {config.min_periods} observations\")\n",
    "print(f\"\\nRationale:\")\n",
    "print(f\"  - 20-day lookback captures tactical credit signal dynamics\")\n",
    "print(f\"  - Minimum 10 observations ensures statistical validity\")\n",
    "print(f\"  - Z-score normalization for regime independence\")\n",
    "print(f\"\\n✓ Configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0616b6a",
   "metadata": {},
   "source": [
    "## 5. Compute Signals via Registry\n",
    "\n",
    "Execute enabled signals using batch computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1eac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPUTING SIGNALS\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'market_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Compute all enabled signals\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     signals_dict = compute_registered_signals(registry, \u001b[43mmarket_data\u001b[49m, config)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Successfully computed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(signals_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m signals\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameError\u001b[39m: name 'market_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPUTING SIGNALS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Compute all enabled signals\n",
    "try:\n",
    "    signals_dict = compute_registered_signals(registry, market_data, config)\n",
    "    print(f\"\\n✓ Successfully computed {len(signals_dict)} signals\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\n✗ Data requirement error: {e}\")\n",
    "    raise\n",
    "except AttributeError as e:\n",
    "    print(f\"\\n✗ Compute function error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verify expected number of signals\n",
    "expected_count = len(enabled_signals)\n",
    "actual_count = len(signals_dict)\n",
    "\n",
    "if actual_count != expected_count:\n",
    "    raise ValueError(\n",
    "        f\"Signal count mismatch: expected {expected_count}, got {actual_count}\"\n",
    "    )\n",
    "\n",
    "# Display per-signal statistics\n",
    "print(f\"\\n\\nSignal Statistics:\\n\")\n",
    "stats_data = []\n",
    "for name, series in signals_dict.items():\n",
    "    stats_data.append({\n",
    "        'Signal': name,\n",
    "        'Valid Obs': series.notna().sum(),\n",
    "        'Mean': f\"{series.mean():.3f}\",\n",
    "        'Std': f\"{series.std():.3f}\",\n",
    "        'Min': f\"{series.min():.2f}\",\n",
    "        'Max': f\"{series.max():.2f}\",\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "print(stats_df.to_markdown(index=False))\n",
    "\n",
    "# Combine into single DataFrame (preserves catalog order)\n",
    "signals = pd.DataFrame(signals_dict)\n",
    "\n",
    "print(f\"\\n\\nCombined Signals DataFrame:\")\n",
    "print(f\"  Shape: {signals.shape}\")\n",
    "print(f\"  Columns: {list(signals.columns)}\")\n",
    "print(f\"  Index: {signals.index.name} ({len(signals)} dates)\")\n",
    "print(f\"  Date range: {signals.index.min()} to {signals.index.max()}\")\n",
    "\n",
    "print(f\"\\n✓ Signals computed and combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0432562",
   "metadata": {},
   "source": [
    "## 6. Validate Signal Properties\n",
    "\n",
    "Check z-score normalization, alignment, and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96078959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SIGNAL VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Check 1: Z-Score Normalization\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'signals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Check 1: Z-score normalization\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCheck 1: Z-Score Normalization\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43msignals\u001b[49m.columns:\n\u001b[32m     11\u001b[39m     mean = signals[col].mean()\n\u001b[32m     12\u001b[39m     std = signals[col].std()\n",
      "\u001b[31mNameError\u001b[39m: name 'signals' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SIGNAL VALIDATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "validation_results = []\n",
    "all_passed = True\n",
    "\n",
    "# Check 1: Z-score normalization\n",
    "print(\"Check 1: Z-Score Normalization\\n\")\n",
    "for col in signals.columns:\n",
    "    mean = signals[col].mean()\n",
    "    std = signals[col].std()\n",
    "    mean_ok = -0.3 <= mean <= 0.3\n",
    "    std_ok = 0.7 <= std <= 1.3\n",
    "    passed = mean_ok and std_ok\n",
    "    \n",
    "    validation_results.append({\n",
    "        'Check': 'Z-Score Normalization',\n",
    "        'Signal': col,\n",
    "        'Status': '✓ PASS' if passed else '✗ FAIL',\n",
    "        'Details': f\"mean={mean:.3f}, std={std:.3f}\",\n",
    "    })\n",
    "    \n",
    "    if not passed:\n",
    "        all_passed = False\n",
    "        print(f\"  ⚠️  {col}: mean={mean:.3f} (target: ±0.3), std={std:.3f} (target: 0.7-1.3)\")\n",
    "\n",
    "if all([r['Status'] == '✓ PASS' for r in validation_results if r['Check'] == 'Z-Score Normalization']):\n",
    "    print(\"  ✓ All signals properly normalized\\n\")\n",
    "\n",
    "# Check 2: DatetimeIndex alignment\n",
    "print(\"Check 2: DatetimeIndex Alignment\\n\")\n",
    "aligned = signals.index.equals(cdx_df.index)\n",
    "validation_results.append({\n",
    "    'Check': 'Index Alignment',\n",
    "    'Signal': 'All',\n",
    "    'Status': '✓ PASS' if aligned else '✗ FAIL',\n",
    "    'Details': f\"signals.index == cdx_df.index: {aligned}\",\n",
    "})\n",
    "\n",
    "if aligned:\n",
    "    print(f\"  ✓ Signals aligned with CDX index ({len(signals)} dates)\\n\")\n",
    "else:\n",
    "    all_passed = False\n",
    "    print(f\"  ✗ Index mismatch detected\\n\")\n",
    "\n",
    "# Check 3: Correlation matrix\n",
    "print(\"Check 3: Signal Correlations\\n\")\n",
    "corr_matrix = signals.corr()\n",
    "\n",
    "# Check for excessive correlation (>0.9 indicates redundancy)\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_val = corr_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.9:\n",
    "            high_corr.append(f\"{corr_matrix.columns[i]} vs {corr_matrix.columns[j]}: {corr_val:.3f}\")\n",
    "\n",
    "corr_ok = len(high_corr) == 0\n",
    "validation_results.append({\n",
    "    'Check': 'Correlation Range',\n",
    "    'Signal': 'All',\n",
    "    'Status': '✓ PASS' if corr_ok else '⚠️  WARNING',\n",
    "    'Details': f\"High correlations (>0.9): {len(high_corr)}\",\n",
    "})\n",
    "\n",
    "if corr_ok:\n",
    "    print(f\"  ✓ No excessive correlations detected\\n\")\n",
    "else:\n",
    "    print(f\"  ⚠️  High correlations found:\")\n",
    "    for item in high_corr:\n",
    "        print(f\"    {item}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Correlation Matrix:\\n\")\n",
    "print(corr_matrix.to_string(float_format='%.3f'))\n",
    "\n",
    "# Display validation summary\n",
    "print(f\"\\n\\nValidation Summary:\\n\")\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "print(validation_df.to_markdown(index=False))\n",
    "\n",
    "if all_passed:\n",
    "    print(f\"\\n✓ All validation checks passed\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Some validation checks failed - review warnings above\")\n",
    "    print(f\"Signal computation will continue, but results should be reviewed carefully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348bb68",
   "metadata": {},
   "source": [
    "## 7. Visualize Signal Time Series\n",
    "\n",
    "Plot individual signals and comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d0a102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SIGNAL VISUALIZATION\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'signals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Plot each signal individually\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m signal_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43msignals\u001b[49m.columns:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlotting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msignal_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     fig = plot_signal(\n\u001b[32m      9\u001b[39m         signals[signal_name],\n\u001b[32m     10\u001b[39m         title=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSignal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msignal_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         threshold_lines=[-\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m],\n\u001b[32m     12\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'signals' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SIGNAL VISUALIZATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Plot each signal individually\n",
    "for signal_name in signals.columns:\n",
    "    print(f\"Plotting {signal_name}...\")\n",
    "    fig = plot_signal(\n",
    "        signals[signal_name],\n",
    "        title=f\"Signal: {signal_name}\",\n",
    "        threshold_lines=[-2, 2],\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "print(f\"\\n✓ Individual signal plots complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ea782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3-panel comparison subplot\n",
    "print(f\"\\nCreating signal comparison subplot...\")\n",
    "\n",
    "signal_names = list(signals.columns)\n",
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=1,\n",
    "    subplot_titles=signal_names,\n",
    "    vertical_spacing=0.08,\n",
    ")\n",
    "\n",
    "colors = ['steelblue', 'darkorange', 'darkgreen']\n",
    "\n",
    "for i, signal_name in enumerate(signal_names, start=1):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=signals.index,\n",
    "            y=signals[signal_name],\n",
    "            name=signal_name,\n",
    "            line=dict(color=colors[i-1], width=1.5),\n",
    "        ),\n",
    "        row=i,\n",
    "        col=1,\n",
    "    )\n",
    "    \n",
    "    # Add threshold lines\n",
    "    for threshold in [-2, 2]:\n",
    "        fig.add_hline(\n",
    "            y=threshold,\n",
    "            line_dash=\"dot\",\n",
    "            line_color=\"red\",\n",
    "            opacity=0.4,\n",
    "            row=i,\n",
    "            col=1,\n",
    "        )\n",
    "    \n",
    "    # Add zero line\n",
    "    fig.add_hline(\n",
    "        y=0,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"gray\",\n",
    "        opacity=0.5,\n",
    "        row=i,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    title_text=\"Signal Comparison - All Signals\",\n",
    "    showlegend=False,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\")\n",
    "fig.update_yaxes(title_text=\"Signal Value\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"✓ Comparison subplot complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca950cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "print(f\"\\nCreating correlation heatmap...\")\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    text_auto=True,\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    aspect=\"auto\",\n",
    "    title=\"Signal Correlation Matrix\",\n",
    "    labels=dict(color=\"Correlation\"),\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=500,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"✓ Correlation heatmap complete\")\n",
    "print(f\"\\n✓ All visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec6195",
   "metadata": {},
   "source": [
    "## 8. Persist Signals and Metadata\n",
    "\n",
    "Save signals DataFrame and computation metadata for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PERSISTING OUTPUTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Save signals DataFrame\n",
    "signals_path = DATA_DIR / \"processed\" / \"signals.parquet\"\n",
    "save_parquet(signals, signals_path)\n",
    "\n",
    "signals_size_mb = signals_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"✓ Signals saved to: {signals_path}\")\n",
    "print(f\"  Size: {signals_size_mb:.2f} MB\")\n",
    "print(f\"  Shape: {signals.shape}\")\n",
    "\n",
    "# Create computation metadata\n",
    "metadata = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"lookback\": config.lookback,\n",
    "        \"min_periods\": config.min_periods,\n",
    "    },\n",
    "    \"date_range\": {\n",
    "        \"start\": signals.index.min().isoformat(),\n",
    "        \"end\": signals.index.max().isoformat(),\n",
    "    },\n",
    "    \"signal_names\": list(signals.columns),\n",
    "    \"observation_count\": len(signals),\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = LOGS_DIR / \"signal_computation_metadata.json\"\n",
    "save_json(metadata, metadata_path)\n",
    "\n",
    "metadata_size_kb = metadata_path.stat().st_size / 1024\n",
    "print(f\"\\n✓ Metadata saved to: {metadata_path}\")\n",
    "print(f\"  Size: {metadata_size_kb:.2f} KB\")\n",
    "\n",
    "print(f\"\\n✓ All outputs persisted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b830754",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workflow Complete\n",
    "\n",
    "Signal computation successful! The computed signals are now ready for suitability evaluation in Step 3.\n",
    "\n",
    "### What Was Accomplished\n",
    "\n",
    "✓ **Market Data Loaded** — CDX, VIX, and ETF data from Bloomberg cache  \n",
    "✓ **Signals Computed** — All enabled signals via SignalRegistry  \n",
    "✓ **Validation Passed** — Z-score normalization and alignment verified  \n",
    "✓ **Visualizations Created** — Time series and correlation analysis  \n",
    "✓ **Outputs Persisted** — Signals and metadata saved for reproducibility\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "```\n",
    "Cached Market Data (Step 1)\n",
    "    ↓\n",
    "Signal Computation (this notebook)\n",
    "    ↓\n",
    "signals: pd.DataFrame\n",
    "├─ cdx_etf_basis: z-score normalized\n",
    "├─ cdx_vix_gap: z-score normalized\n",
    "└─ spread_momentum: z-score normalized\n",
    "    ↓\n",
    "Suitability Evaluation (next notebook)\n",
    "```\n",
    "\n",
    "### Re-Running This Notebook\n",
    "\n",
    "- **Data source:** Loads from cache created in Step 1\n",
    "- **Recomputation:** Signals are recomputed from scratch each run\n",
    "- **Outputs:** Overwrites `signals.parquet` and metadata JSON\n",
    "- **Catalog changes:** Edit `signal_catalog.json` to enable/disable signals\n",
    "\n",
    "### Key Files Generated\n",
    "\n",
    "```\n",
    "data/\n",
    "└── processed/\n",
    "    └── signals.parquet (multi-column DataFrame)\n",
    "\n",
    "logs/\n",
    "└── signal_computation_metadata.json\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Cache files not found:**\n",
    "- Run `01_data_download.ipynb` first\n",
    "- Verify cache directory: `data/cache/bloomberg/`\n",
    "- Check for files: `cdx_cdx_ig_5y.parquet`, `vix_vix.parquet`, `etf_hyg.parquet`\n",
    "\n",
    "**Signal computation errors:**\n",
    "- Check data requirements in signal catalog\n",
    "- Verify market_data dict has correct keys: cdx, vix, etf\n",
    "- Review ERROR logs for missing columns or functions\n",
    "\n",
    "**Validation warnings:**\n",
    "- Z-score normalization may vary with data regime\n",
    "- Minor deviations (mean ±0.5, std 0.5-1.5) are acceptable\n",
    "- Review signal statistics and proceed if reasonable\n",
    "\n",
    "**Visualization errors:**\n",
    "- Ensure visualization dependencies installed: `uv sync --extra viz`\n",
    "- Check plotly import succeeds in first cell\n",
    "- Verify Jupyter can render plotly figures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
